{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Stick Balancing Robot\n",
    "## Reinforcement Learning and Robotics\n",
    "### Team: Alex Reichenbach, Gherardo Morona, Alex Davenport and Nhat Pham"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "import pandas as pdOA\n",
    "import numpy as np\n",
    "import random\n",
    "# import serial\n",
    "# import rospy\n",
    "# import roslib\n",
    "# from geometry_msgs.msg import Twist\n",
    "# ---> commened import statements are for the robot\n",
    "import pickle\n",
    "from threading import Thread\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Summary\n",
    "Our goal in this project was to balance an inverted pendulum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/0/00/Cart-pendulum.svg/300px-Cart-pendulum.svg.png\" style=\"width:200px; height:200px; float:left\"> <img src=\"robot.png\" style=\"width:200px; height:200px; float:right\">'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This problem is most naturally suited to reinforcement learning because there is no previous dataset, but rather a reward function. It's good if the pendulum is pointed up, and it's bad if it's at an angle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We originally tried to create a neural network, but that proved overcomplicated for this situation. This led us to attempt Q-learning with a tabular method of reinforcement learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interfacing with the Robot\n",
    "The turtlebot uses ROS (Robot Operating System). Then it operates on a publisher subscriber system. Unfortunately, this system can cause lags and loss in communication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ser = serial.Serial('/dev/ttyACM0', 9600)\n",
    "# rospy.init_node('goalieBot', anonymous=True)\n",
    "# movement = rospy.Publisher(\"/mobile_base/commands/velocity\", Twist, queue_size=None, tcp_nodelay=True, latch=True)\n",
    "# timePerStep = 8\n",
    "# r = rospy.Rate(timePerStep)\n",
    "# moveCmd = Twist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learner class\n",
    "The Learner class is what allows the robot to use reinforcement learning. This class receives the reward and state. It then decides the action which should be taken next in order to maximize the expected reward. Then, it uses the reward received from that action to update the q-table with the new q-values.\n",
    "\n",
    "The variables used by this class are:\n",
    "* `num_bins`: the number of discrete values we divide our angle measurements\n",
    "* `num_actions`: the number of discrete values divide our possible movements\n",
    "* `q-matrix`: our q-table (with the shape [`num_bins`, `num_bins`, `num_actions`])\n",
    "* `state`: a 2D array with one axis representing the previous angle measures and the other representing the current angle measures. Having both measures allowed us to take into account the angular velocity of the inverted pendulum\n",
    "* `actionIndex`: the index in our `q-matrix` of the action that would maximize our expected reward\n",
    "\n",
    "Hyperparameters:\n",
    "* `epsilon`: the probability of making a random action\n",
    "* `gamma`: the importance of the future rewards\n",
    "* `alpha`: learning rate of the robot\n",
    "* `epsilonDecay`: decreases the need of randomness as the model becomes more trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Learner:\n",
    "    num_bins = 10\n",
    "    num_actions = 30\n",
    "    \n",
    "    def __init__(self):\n",
    "        num_states = 2\n",
    "        self.qmatrix = np.zeros((Learner.num_bins+1, Learner.num_bins+1, Learner.num_actions+1))\n",
    "        self.epsilon = .7\n",
    "        self.gamma = .75\n",
    "        self.alpha = .8\n",
    "        self.epsilonDecay = .99\n",
    "        self.state = [0, 0]\n",
    "        self.actionIndex = 0\n",
    "\n",
    "    # This method takes newState (the state where the pendulum is currently in) as well as the reward it has received.\n",
    "    # it updates the q-table with the q-value which the actions it just took should have\n",
    "    # it returns newActionIndex, the index of the next action the robot should take\n",
    "    def getMove(self, newState, reward):\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.randint(0,Learner.num_actions)\n",
    "        newActionIndex = self.qmatrix[newState[0]][newState[1]].argsort()[-1]\n",
    "        #alpha = 1\n",
    "        self.qmatrix[int(self.state[0])][int(self.state[1])][int(self.actionIndex)] = float(self.qmatrix[self.state[0]][self.state[1]][self.actionIndex]) + \\\n",
    "                                    reward + self.gamma * float(self.qmatrix[int(newState[0])][int(newState[1])][int(newActionIndex)])\n",
    "        self.actionIndex = newActionIndex\n",
    "        self.state = newState\n",
    "        self.epsilon *= self.epsilonDecay\n",
    "        return newActionIndex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation\n",
    "Below are some of the methods we use to gather angular data from the Arduino, make the reward computations, and then send commands the Robot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "speed = 0\n",
    "reward = 0\n",
    "\n",
    "# Turn a value into its nearest bin\n",
    "def to_bin(value, bins):\n",
    "    return np.digitize(x=[value], bins=bins)[0]\n",
    "\n",
    "# Keep reading from serial until a real number is read.\n",
    "# Necessary because Serial isn't always reliable in its reading.\n",
    "def getAngle():\n",
    "    global speed\n",
    "    a=float('inf')\n",
    "    while a == float('inf'):\n",
    "        try:\n",
    "            a = int(ser.readline().decode().strip())\n",
    "        except:\n",
    "            print(\"Failed reading...Trying again...\")\n",
    "    return a\n",
    "\n",
    "# Execute action in the environment\n",
    "def makeMove(action):\n",
    "    global moveCmd\n",
    "    moveCmd = Twist()\n",
    "    moveCmd.linear.x = action\n",
    "    movement.publish(moveCmd)\n",
    "\n",
    "# Gets a reward - we tried two different methods.\n",
    "# This method turned out to be real trouble.\n",
    "def getReward():\n",
    "    global reward\n",
    "    \n",
    "    # Method 1 - get the highest score that is obtained\n",
    "    # in between each timestep. In case by the time it reads\n",
    "    # the pendulum has already gone up and fallen back down.\n",
    "    # The reward is reset to -1 in each step below\n",
    "    reward = max(reward, 100/getAngle())\n",
    "    \n",
    "    # Method 2 - keep a running average of the past 10 angles.\n",
    "    # Prevents faulty readings from causing catostrophic forgetting.\n",
    "    rewardArr = [0 for x in range(10)]\n",
    "    while True:\n",
    "        del rewardArr[0]\n",
    "        rewardArr.append(1000/(abs(getAngle())+1))\n",
    "        reward = np.sum(rewardArr)/len(rewardArr)\n",
    "        time.sleep(.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the `main` function which cycles through the learning process, calling the appropriate methods written above.\n",
    "\n",
    "We are not going to differentiate between episode and step because, when we train the robot, we let it run without ever stopping it at each episode (i.e. moment when robot fails).\n",
    "\n",
    "In Reinforcement terminology: Our failure state is the same as our starting state, so we do not need the reset stage that occurs in between each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    global speed, reward\n",
    "    learner = Learner()\n",
    "    # Create discrete bins for state and action\n",
    "    angleBins = np.array(list(np.linspace(-60,70,Learner.num_bins)))\n",
    "    actionBins = np.array(list(np.linspace(-1.5,1.5,Learner.num_actions+1)))\n",
    "    previousAngle = getAngle()\n",
    "\n",
    "    try:\n",
    "        print(\"Trying to load matrix...\", end='')\n",
    "        learner.qmatrix = pickle.load(open(\"qMatrix.pkl\", 'rb'))\n",
    "        print(\"Success!\")\n",
    "    except:\n",
    "        print(\"Failed.\")\n",
    "    \n",
    "    thread = Thread(target = getReward) # asynchronously start reward function to allow further precision in it.\n",
    "    thread.start()\n",
    "    \n",
    "    for step in range(10000):\n",
    "        currentAngle = getAngle()\n",
    "        state = [to_bin(currentAngle, angleBins), to_bin(previousAngle, angleBins)] # get current angle and put it in bin\n",
    "        print(\"Reward:\", reward)\n",
    "        action = actionBins[learner.getMove(state, reward)] # decide on the action\n",
    "        reward = -1 # necessary for method 1 of the reward function\n",
    "        makeMove(action)\n",
    "        previousAngle = currentAngle\n",
    "        time.sleep(1/timePerStep) # Sleep so the robot has time to execute action in between steps\n",
    "        #Save the Q-Matrix in between epochs.\n",
    "        if step%5 == 0:\n",
    "            print(\"Saving the q-matrix\")\n",
    "            pickle.dump(learner.qmatrix, open(\"qMatrix.pkl\", 'wb'))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation\n",
    "The code worked within a regulated simulation where the real world troubles didn't happen.\n",
    "<html>\n",
    "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/MgILWUAxUQQ\" frameborder=\"0\"></iframe>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Steps\n",
    "\n",
    "There are a couple ways we could proceed moving forward given more time. \n",
    "\n",
    "It might be better to implement acceleration as the action instead of velocity. We were unable to do it with our resources as the robot wasn't computing the next move fast enough for it to be useful. \n",
    "\n",
    "Secondly, the robot wasn't ideal for the task. In the perfect world we would use a cart on a track so there is no movement in another axis and it is more stable.\n",
    " \n",
    "Thirdly, we might be able to improve the simulation and make it accurate to the point where we can train on the simulation and use the same table on the robot. However the two are not compatible presently and we can only use it to confirm our code works."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
